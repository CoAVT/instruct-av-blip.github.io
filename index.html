
<html>
    <head>
    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,900' rel='stylesheet' type='text/css'>
    <link href="style.css" rel="stylesheet">
    <title>Instruct-AV-BLIP</title>
    </head>
    <body>

        <div id="header" class="container-fluid">
            <div class="row" style="text-align: center;"/>
            <!--<div class="logoimg">
                <img src="meta_logo.png" height="95">
                <img src="huji_logo.png" height="95">
            </div>-->
                <h1>Instruct-AV-BLIP</h1>
                    <div class="authors">
                    <a href="">Xianghu Yue</a><sup>1, 2</sup>,
                    <a href="">Xiaohai Tian</a><sup>2</sup>,
                    <a href="">Lu Lu</a><sup>2</sup>,
                    <a href="">Zhizheng Wu</a><sup>1</sup>,
                    <a href="">Haizhou Li</a><sup>1, 2</sup>,
                </div>

                <br>
                <p><sup>1</sup>The Chinese University of Hong Kong, Shenzhen <sup>2</sup>Natioanl University of Singapore <sup>3</sup>ByteDance AI Lab</p>

                <div class="assets">
                    <a href="https://arxiv.org/abs/2401.12264" target="_blank">[Paper]</a>
                </div>
                
        </div>
    
    <div class="container">
        <h2>Abstract</h2>
        <span>
            There has been a long-standing quest for a unified audio-visual-text model to enable various multimodal understanding tasks, which mimics the listening, seeing and reading process of human beings. Humans tends to represent knowledge using two separate systems: one for representing verbal (textual) information and one for representing non-verbal (visual and auditory) information. These two systems can operate independently but can also interact with each other. Motivated by this understanding of human cognition, in this paper, we introduce CoAVT -- a novel cognition-inspired Correlated Audio-Visual-Text pre-training model to connect the three modalities. It contains a joint audio-visual encoder that learns to encode audio-visual synchronization information together with the audio and visual content for non-verbal information, and a text encoder to handle textual input for verbal information. To bridge the gap between modalities, CoAVT employs a query encoder, which contains a set of learnable query embeddings, and extracts the most informative audiovisual features of the corresponding text. Additionally, to leverage the correspondences between audio and vision with language respectively, we also establish the audio-text and visual-text bi-modal alignments upon the foundational audiovisual-text tri-modal alignment to enhance the multimodal representation learning. Finally, we jointly optimize CoAVT model with three multimodal objectives: contrastive loss, matching loss and language modeling loss. Extensive experiments show that CoAVT can learn strong multimodal correlations and be generalized to various downstream tasks. CoAVT establishes new state-of-the-art performance on text-video retrieval task on AudioCaps for both zero-shot and fine-tuning settings, audio-visual event classification and audio-visual retrieval tasks on AudioSet and VGGSound.The results demonstrate the effectiveness and superiority of the proposed model for multimodal processing.
        </span>
    </div>





    </body>
    </html>
    
